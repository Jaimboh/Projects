{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "OpenAI is a leading player in the Artificial Intelligence industry and has made numerous AI models, such as GPT and CLIP, available to the public.\n",
        "\n",
        "OpenAI has open-sourced the Whisper models, which have achieved near human-level performance and accuracy in English speech recognition.\n",
        "\n",
        "This project will be a process of converting audio into text using OpenAI's Whisper and the HuggingFace Transformers framework.\n",
        "\n",
        "Upon completion of this project, hope we will have the ability to transcribe both English and non-English audio into text\n",
        "## OpenAIâ€™s Whisper\n",
        "Whisper models have been developed to study the capability of speech-processing systems for speech recognition and translation tasks. They have the capability of transcribing speech audio into text.\n",
        "\n",
        "Trained on 680,000 hours of labeled audio data, which is reported by the authors to be one of the largest ever created in supervised speech recognition. Also, the model's performance has been evaluated by training a series of medium-sized models on subsampled versions of the data corresponding to 0.5%, 1%, 2%, 4%, and 8% of the full dataset\n"
      ],
      "metadata": {
        "id": "nS2_kqhs2EiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  installing and importing the relevant modules to implementing the audio transcription and translation cases"
      ],
      "metadata": {
        "id": "-bnF602i3iQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moURaplxxkFt",
        "outputId": "a36df881-dd08-4952-ddf6-e3c7570653a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-0wqfc2ch\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-0wqfc2ch\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (4.0.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1179424 sha256=af3a9f177daede97808797316a0b1805baa9691cbbc0ef5070cad81e23628246\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f7jkd_9m/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, transformers, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.12.0 openai-whisper-20230124 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the nvidia-smi we can have the information about the GPU allocated to you, and here is mine."
      ],
      "metadata": {
        "id": "59QcYMNj6F9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocJtYYrP4jXF",
        "outputId": "30638e87-ae37-4d67-e03a-3fd3f74d4450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb  3 15:30:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    31W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have everything installed, we can import the modules and load the model. In our case, we will be using the large model which has 1550M parameters and requires ~10Gigabyte VRAM memory. The processing can be longer or faster whether you are using a CPU or a GPU."
      ],
      "metadata": {
        "id": "3c1MmBW76Q4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries \n",
        "import whisper\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Initialize the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model \n",
        "whisper_model = whisper.load_model(\"large\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkU0dgf35w7q",
        "outputId": "f62e284f-b242-4600-e654-4210f2a8b104"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.87G/2.87G [00:23<00:00, 130MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the load_model() function, we use the device initiated in the line before. By default, the newly created tensors are created on the CPU if not specified otherwise"
      ],
      "metadata": {
        "id": "Mn-W6Vb96os4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is the time to start extracting audio filesâ€¦"
      ],
      "metadata": {
        "id": "BhQnL8KD6rbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Transcription\n",
        "We need to install the pytube library using the following pip statement to download the audio from YouTube"
      ],
      "metadata": {
        "id": "Uh8G8_pT6uea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the module\n",
        "!pip install pytube\n",
        "\n",
        "# Import the module\n",
        "from pytube import YouTube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ7Vmn5h67zQ",
        "outputId": "1ed812ca-6cd4-426d-d480-a15f5a095ba2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-12.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can implement the helper function as follows:"
      ],
      "metadata": {
        "id": "mfCqfAmM7CP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def video_to_audio(video_URL, destination, final_filename):\n",
        "\n",
        "  # Get the video\n",
        "  video = YouTube(video_URL)\n",
        "\n",
        "  # Convert video to Audio\n",
        "  audio = video.streams.filter(only_audio=True).first()\n",
        "\n",
        "  # Save to destination\n",
        "  output = audio.download(output_path = destination)\n",
        "\n",
        "  _, ext = os.path.splitext(output)\n",
        "  new_file = final_filename + '.mp3'\n",
        "\n",
        "  # Change the name of the file\n",
        "  os.rename(output, new_file)"
      ],
      "metadata": {
        "id": "708gHenI7GoY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function takes three parameters:\n",
        "\n",
        "* video_URL the full URL of the YouTube video.\n",
        "* destination the location where to save the final audio.\n",
        "* final_filename the name to give to the final audio.\n",
        "\n",
        "Finally, we can use the function to download the video and convert it into audio."
      ],
      "metadata": {
        "id": "XzhIoZAg7XhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English transcription\n",
        "The video used here is a 30 seconds motivational speech on YouTube from Motivation Quickie. Only the first 17 seconds correspond to the true speech and the rest of the speech is noise."
      ],
      "metadata": {
        "id": "5s46Jx6E7gC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Video to Audio\n",
        "video_URL = 'https://www.youtube.com/watch?v=E9lAeMz1DaM'\n",
        "destination = \".\"\n",
        "final_filename = \"motivational_speech\"\n",
        "video_to_audio(video_URL, destination, final_filename)\n",
        "\n",
        "# Audio to text\n",
        "audio_file = \"motivational_speech.mp3\"\n",
        "result = whisper_model.transcribe(audio_file)\n",
        "\n",
        "# Print the final result\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycbhhqFk7u8H",
        "outputId": "4905a089-4198-4e7a-dfbb-b0764994cc5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I don't know what that dream is that you have. I don't care how disappointing it might have been as you've been working toward that dream. But that dream that you're holding in your mind, that it's possible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* videoURL is the link to the motivational speech.\n",
        "* destination is my current folder corresponding to `. `\n",
        "* motivational_speech will be the final name of the audio.\n",
        "* whisper_model.transcribe(audio_file) applies the model on the audio file to generate the transcription.\n",
        "* The transcribe()function preprocess the audio with a sliding 30-second window, and performs an autoregressive sequence-to-sequence approach to make predictions on each window.\n",
        "*Finally, the print() statement generates the results."
      ],
      "metadata": {
        "id": "kNx-NnOn8Cd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the link to the video:https://www.youtube.com/watch?v=E9lAeMz1DaM"
      ],
      "metadata": {
        "id": "OORueAiR8dCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-English transcription\n",
        "In addition to English, Whisper can also deal with non-English languages. Letâ€™s have a look at Alassane Dramane Ouattaraâ€™s interview on YouTube.\n",
        "\n",
        "Similarly to the previous approach, we get the video, translate it to audio and get the content."
      ],
      "metadata": {
        "id": "RRFcEyKN820N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/watch?v=D8ztTzHHqiE\"\n",
        "destination = \".\"\n",
        "final_filename = \"discours_ADO\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"discours_ADO.mp3\"\n",
        "result_ADO = whisper_model.transcribe(audio_file)\n",
        "\n",
        "# Show the result\n",
        "print(result_ADO[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifKx8RCe8X7O",
        "outputId": "c359c6a4-2ee9-4756-d25e-6377e9b88fac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " â€” Le franc CFA, vous l'avez toujours dÃ©fendu, Beck et Ong. Est-ce que vous continuez Ã  le faire ou est-ce que vous pensez qu'il faut peut-Ãªtre changer les choses sans rentrer trop dans les dÃ©tails techniques? â€” M. Perelman, je vous dirais tout simplement qu'il y a vraiment du n'importe quoi dans ce dÃ©bat. Moi, je vais pas manquer de modestie. Mais j'ai Ã©tÃ© directeur des Ã©tudes de la Banque centrale, j'ai Ã©tÃ© vice-gouverneur, j'ai Ã©tÃ© gouverneur de la Banque centrale. Donc je peux vous dire que je sais de quoi je parle. Le franc CFA, c'est notre monnaie. C'est la monnaie des pays membres. Et nous l'avons acceptÃ©e et nous l'avons dÃ©veloppÃ©e, nous l'avons modifiÃ©e. Et j'Ã©tais lÃ  quand la rÃ©forme a eu lieu dans les annÃ©es 73-74. Alors donc tout ce dÃ©bat est un non-sens. Maintenant, c'est notre monnaie. J'ai quand mÃªme eu Ã  superviser la gestion monÃ©taire et financiÃ¨re de plus de 120 pays dans le monde quand j'Ã©tais au Fonds monÃ©taire international. Mais je suis bien placÃ© pour dire que si cette monnaie nous pose problÃ¨me, Ã©coutez, avec les autres chefs d'Ã‰tat, nous prendrons les dÃ©cisions. Mais cette monnaie est solide. Elle est appropriÃ©e. Les taux de croissance sont parmi les plus Ã©levÃ©s sur le continent africain et mÃªme dans le monde. La CÃ´te d'Ivoire est parmi les 10 pays oÃ¹ le taux de croissance est le plus Ã©levÃ©. Donc c'est un non-sens tout simplement de la dÃ©magogie et je ne souhaite mÃªme pas continuer ce dÃ©bat sur le franc CFA. C'est la monnaie des pays africains qui ont librement consenti et acceptÃ© de se mettre ensemble. Bien sÃ»r, chacun de nous aurait pu avoir sa monnaie, mais quel serait l'intÃ©rÃªt? Pourquoi les EuropÃ©ens ont dÃ©cidÃ© d'avoir une monnaie commune et que nous, les Africains, ne serons pas en mesure de le faire? Nous sommes trÃ¨s fiers de cette monnaie. Elle marche bien. S'il y a des adaptations Ã  faire, nous le ferons de maniÃ¨re souveraine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the final result, and the result is mindblowing ðŸ¤¯"
      ],
      "metadata": {
        "id": "9Q82JeA9ABB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-English transcription into English\n",
        "In addition to speech recognition, spoken language identification, and voice activity identification, Whisper is also able to perform speech translation from any language into English."
      ],
      "metadata": {
        "id": "v7AGHPnMAF3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/watch?v=D8ztTzHHqiE\"\n",
        "final_filename = \"discours_ADO\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"discours_ADO.mp3\"\n",
        "french_to_english = whisper_model.transcribe(audio_file, task = 'translate')\n",
        "\n",
        "# Show the result\n",
        "print(french_to_english[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EnT-a9jAzxy",
        "outputId": "a153f8f9-4eeb-49e3-da98-08ac879f609a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " France CFA, you have always defended it, Beck et Ongle, do you continue to do so or do you think that perhaps things need to be changed without going into too much technical details? Mr Perelman, I will simply tell you that there is really nonsense in this debate. I don't want to lack modesty, but I was director of the central bank's studies, I was vice-governor, I was governor of the central bank, so I can tell you that I know what I am talking about. France CFA is our currency, it is the currency of the member states, and we have accepted it, we have developed it, we have modified it. I was there when the reform took place in the years 1973-74, so all this debate is nonsense. Now, it is our currency. I have supervised the financial and monetary management of more than 120 countries in the world. When I was at the International Monetary Fund, I was well placed to say that if this currency poses a problem, we will listen to the other heads of state and make the decisions, but this currency is solid, it is appropriate. Growth rates are among the highest on the African continent and even in the world. The Ivory Coast is among the 10 countries where growth rates are the highest. So, it is simply a nonsense of demagogy and I do not even wish to continue this debate on France CFA. It is the currency of the African countries that have freely agreed and accepted to join forces. Of course, each of us could have had our own currency, but what would be the interest? Why did the Europeans decide to have a common currency and we Africans will not be able to do so? We are very proud of this currency. It works well. If there are adaptations to be made, we will do so in a sovereign way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "task=â€™translateâ€™means that we are performing a translation task"
      ],
      "metadata": {
        "id": "CcX0EKcQBo5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In conclusion, the \"Speech-to-Text and Translation with OpenAI's Whisper\" project demonstrated how to use OpenAI's Whisper models and the HuggingFace Transformers framework to transcribe audio into text and translate it into other languages. The Whisper models are open-sourced by OpenAI and are considered to have achieved near human-level performance and accuracy in English speech recognition. This project aimed to provide a comprehensive guide to help individuals harness the power of AI to transform audio into text and translate it into other languages, making the process simple and accessible. By the end of the project, we have had a clear understanding of how to perform speech-to-text and machine translation using OpenAI's Whisper models."
      ],
      "metadata": {
        "id": "_rvQrz77BqBN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXcEKf4IHP5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}